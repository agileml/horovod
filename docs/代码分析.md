MPI，OpenMPI 与深度学习（https://zhuanlan.zhihu.com/p/158584571）对Horovod代码流程做了简单分析。

“Kungfu: A Novel Distributed Training System for TensorFlow using Flexible Synchronisation”一文，在附录中，列出了主要逻辑步骤，比较清晰，摘录如下。

The following provides a detailed explanation of the implementation  of Horovod confirming the observations made in the scalability experiments. These insights are made based on analysis of the publicly available Horovod source code.

As seen in the Horovod Timeline, the negotiation phase is the most expensive. We will focus on NEGOTIATE_ALLREDUCE, which is done for all gradient tensors computed during a training iteration. The distributed setup employed by Horovod consists of workers, one of which (Rank 0) plays the role of master.

As seen in the Horovod Timeline, the negotiation phase is the most expensive. We will focus on NEGOTIATE_ALLREDUCE, which is done for all gradient tensors computed during a training iteration. The distributed setup employed by Horovod consists of workers, one of which (Rank 0) plays the role of master. 

Horovod runs the same copy of the training process on multiple 4-GPU machines, using a distinctly shuffled dataset per training process. After each forward pass through the neural network, the weight updates computed using gradient descent are back-propagated. Before each weight tensor is updated locally, the global negotiation phase is triggered, resulting in computation of theaverage of all local gradients. The steps followed to compute  all-reduce with operator MPI_SUM on tensori are the following:

+ Worker sends MPI_Request for tensori to master.

+ After worker sends the MPI_Request for all trainable tensors, it sends a DONE message so that
  master should stop expecting any more messages from that worker.

+ Master updates its message_table, which is a hashmap from tensor name to all timestamped requests received for that tensor. This is done by the function IncrementTensorCount.
    * When the first request for tensori has been received the Horovod Timeline registers the beginning of the negotiation phase for request type ALLREDUCE (start of NEGOTIATE_ALLREDUCE)
* When the size of the requests vector for tensori is equal to MPI_SIZE, i.e. the number of workers running copies of the program, it means that the tensor is ready to be reduced. This is the end of the negotiation.
  

The three-phase message receive operation is synchronous and initiated by the master: get message lengths from every rank (MPI_Gather), compute offsets and collect messages from every rank (MPI_Gatherv, which is the generalized gather version which accounts for an uneven number of messages received from workers).

+ The master creates a vector of tensors ready to be reduced and for each tensor, it constructs an MPI_Response containing also potential error messages related to the worker’s request. 

+ The master performs the optimization called Tensor Fusion, assembling a single stacked response for tensors with the same response type, devices, data type and with a joint size
less than TensorFusionThresholdBytes()

+ The names of the tensors reduced at the current step are broadcast to all workers 

+ The all-reduce operation is executed by the function PerformOperation on all tensors ready to be reduced
  * In the MPI_ALLREDUCE case, all tensors of the TensorTable are copied to the Fusion Buffer. The TensorTable stores TensorTableEntries uniquely identified by tensor name, each containing all necessary data to do the reduction: input tensor, pre-allocated output tensor, the GPU/CPU ID to do reduction on, root rank for broadcast operation, etc
    * The allreduce operation is done in place (MPI_IN_PLACE flag), so the result of the reduction is present in the same buffer at the end of the computation.
  * The resulting tensor is copied to the pre-allocated output Tensor of the TensorTableEntry (See Timeline MEMCPY_IN_FUSION_BUFFER)

+ In the end, the Master performs a stall check on all tensors that were reported as ready for reduction by some ranks (workers), but not by others. Such tensors may cause deadlock of the system.

+ Worker sends to root rank the message length and the message in MPI_Gather and MPI_Gatherv (response operations to the Master-initated operations presented previously). The message is received via the Worker’s local message queue.

+ When the Master has decided that a tensor is ready to be reduced, it sends an MPI_Response to the Worker containing tensor meta-information, received via the MPI_Bcast primitive. The worker performs the operation indicated by the master, which can be one of: MPI Allreduce, NCCL Allreduce or hierarchical all-reduce (MPI & NCCL).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Horovod 代码分析
调用关系
 1. horovod/horovod/torch/__init__.py --> from horovod.torch.mpi_ops import init
 2. horovod/horovod/torch/mpi_ops.py  --> from horovod.common.basics import HorovodBasics as _HorovodBasics
 3. horovod/horovod/common/basics.py  --> HorovodBasics.init()
 4. horovod/horovod/common/operations.cc --> horovod_init
 5. horovod/horovod/common/operations.cc --> InitializeHorovodOnce
 6. horovod/horovod/common/operations.cc --> BackgroundThreadLoop
 7. horovod/horovod/common/operations.cc --> [520] op_manager.reset(CreateOperationManager)
 8. horovod/horovod/common/operations.cc --> CreateOperationManager
 9. horovod/horovod/common/ops/operation_manager.cc --> ExecuteAllreduce
 10. horovod/horovod/common/ops/mpi_operations.cc --> MPIAllreduce::Execute 或其他后端实现
 11. OpenMPI MPI_Allreduce (openmpi-4.0.4/ompi/mca/coll/base/coll_base_allreduce.c)

 


-----------------------------------------------------------------------------
 不管用什么通信后端，都是通过MPI进行negotiation?  rank 0被称为coordinator，它和其它rank进行coordination的过程由Request和Response(horovod/common/message.h中有详细注释)来表示。

 Controller(horovod/common/controller.cpp，controller.h有详细的注释)负责完成negotiation。每个通信后端，都实现了Controller的子类（如MPI有MPIController），但negotiation过程在父类Controller中完成。
	在coordinator node (rank zero)中，通过一个Vector（message_table_变量）保存着所有要进行allreduce的tensor，对某个tensor，当所有rank都同意对它进行allreduce时，就可以开始操作了（Controller::IncrementTensorCount）。
	
 	Allreduce, Allgather and Broadcast三个操作的实现在horovod/common/operations.cc里，支持MPI, NCCL, CUDA/ROCm, Gloo, oneCCL, DDL等通信库。背景线程的实现也在operations.cc。

​	InitializeHorovodOnce：启动背景线程，无论该方法被调用多少次，只能执行一次。MPIController也是在这个方法里创建？	

	horovod_init(horovod/common/operations.cc)->InitializeHorovodOnce(horovod/common/operations.cc)->BackgroundThreadLoop(horovod/common/operations.cc)->RunLoopOnce(horovod/common/operations.cc,后台线程的循环体)->ComputeResponseList(horovod/common/controller.cpp)->IncrementTensorCount(horovod/common/controller.cpp)


​	

​	
​	
​	
---------------------------------------------------------------------------------------
horovod/common/message.h中的Request，定义了RequestType，JOIN只适用于TF(因为operations.cc的EnqueueJoin方法会把一个请求的类型设置为JOIN，而这个方法只在tensorflow/mpi_ops.cc中被调用)?	