MPI，OpenMPI 与深度学习（https://zhuanlan.zhihu.com/p/158584571）对Horovod代码流程做了简单分析。

“Kungfu: A Novel Distributed Training System for TensorFlow using Flexible Synchronisation”一文，在附录中，列出了主要逻辑步骤，比较清晰，摘录如下。

The following provides a detailed explanation of the implementation  of Horovod confirming the observations made in the scalability experiments. These insights are made based on analysis of the publicly available Horovod source code.

As seen in the Horovod Timeline,**the negotiation phase is the most expensive**. We will focus on NEGOTIATE_ALLREDUCE, which is done for all gradient tensors computed during a training iteration. The distributed setup employed by Horovod consists of workers, one of which (Rank 0) plays the role of master.

As seen in the Horovod Timeline, the negotiation phase is the most expensive. We will focus on NEGOTIATE_ALLREDUCE, which is done for all gradient tensors computed during a training iteration. The distributed setup employed by Horovod consists of workers, one of which (Rank 0) plays the role of master. 

Horovod runs the same copy of the training process on multiple 4-GPU machines, using a distinctly shuffled dataset per training process. After each forward pass through the neural network, the weight updates computed using gradient descent are back-propagated. Before each weight tensor is updated locally, the global negotiation phase is triggered, resulting in computation of theaverage of all local gradients. The steps followed to compute  all-reduce with operator MPI_SUM on tensori are the following:

+ Worker sends MPI_Request for tensori to master.

+ After worker sends the MPI_Request for all trainable tensors, it sends a DONE message so that
  master should stop expecting any more messages from that worker.

+ Master updates its message_table, which is a hashmap from tensor name to all timestamped requests received for that tensor. This is done by the function IncrementTensorCount.
    * When the first request for tensori has been received the Horovod Timeline registers the beginning of the negotiation phase for request type ALLREDUCE (start of NEGOTIATE_ALLREDUCE)
* When the size of the requests vector for tensori is equal to MPI_SIZE, i.e. the number of workers running copies of the program, it means that the tensor is ready to be reduced. This is the end of the negotiation.
  

The three-phase message receive operation is synchronous and initiated by the master: get message lengths from every rank (MPI_Gather), compute offsets and collect messages from every rank (MPI_Gatherv, which is the generalized gather version which accounts for an uneven number of messages received from workers).

+ The master creates a vector of tensors ready to be reduced and for each tensor, it constructs an MPI_Response containing also potential error messages related to the worker’s request. 

+ The master performs the optimization called Tensor Fusion, assembling a single stacked response for tensors with the same response type, devices, data type and with a joint size
less than TensorFusionThresholdBytes()

+ The names of the tensors reduced at the current step are broadcast to all workers 

+ The all-reduce operation is executed by the function PerformOperation on all tensors ready to be reduced
  * In the MPI_ALLREDUCE case, all tensors of the TensorTable are copied to the Fusion Buffer. The TensorTable stores TensorTableEntries uniquely identified by tensor name, each containing all necessary data to do the reduction: input tensor, pre-allocated output tensor, the GPU/CPU ID to do reduction on, root rank for broadcast operation, etc
    * The allreduce operation is done in place (MPI_IN_PLACE flag), so the result of the reduction is present in the same buffer at the end of the computation.
  * The resulting tensor is copied to the pre-allocated output Tensor of the TensorTableEntry (See Timeline MEMCPY_IN_FUSION_BUFFER)

+ In the end, the Master performs a stall check on all tensors that were reported as ready for reduction by some ranks (workers), but not by others. Such tensors may cause deadlock of the system.

+ Worker sends to root rank the message length and the message in MPI_Gather and MPI_Gatherv (response operations to the Master-initated operations presented previously). The message is received via the Worker’s local message queue.

+ When the Master has decided that a tensor is ready to be reduced, it sends an MPI_Response to the Worker containing tensor meta-information, received via the MPI_Bcast primitive. The worker performs the operation indicated by the master, which can be one of: MPI Allreduce, NCCL Allreduce or hierarchical all-reduce (MPI & NCCL).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Horovod 代码分析
调用关系

 1. horovod/horovod/torch/__init__.py --> from horovod.torch.mpi_ops import init
 2. horovod/horovod/torch/mpi_ops.py  --> from horovod.common.basics import HorovodBasics as _HorovodBasics
 3. horovod/horovod/common/basics.py  --> HorovodBasics.init()
 4. horovod/horovod/common/operations.cc --> horovod_init
 5. horovod/horovod/common/operations.cc --> InitializeHorovodOnce
 6. horovod/horovod/common/operations.cc --> BackgroundThreadLoop
 7. horovod/horovod/common/operations.cc --> [520] op_manager.reset(CreateOperationManager)
 8. horovod/horovod/common/operations.cc --> CreateOperationManager
 9. horovod/horovod/common/ops/operation_manager.cc --> ExecuteAllreduce
 10. horovod/horovod/common/ops/mpi_operations.cc --> MPIAllreduce::Execute 或其他后端实现
 11. OpenMPI MPI_Allreduce (openmpi-4.0.4/ompi/mca/coll/base/coll_base_allreduce.c)

 


-----------------------------------------------------------------------------
 Horovod operates in two phases:

1. Control Phase: this is when all the ranks decide on which tensors they're going to allreduce. A tensor can only be allreduced if all the ranks have access to a local version of that tensor (which may not be true due to the nature of async processing). The role of the response cache is to avoid all the ranks having to send their full list of "ready" tensors by instead sending a single bit vector. We can compress tensor names to bits because the same tensors appear again and again during training (each batch).
2. Allreduce Phase: once we've decided on which tensors we're going to process, we do the allreduce (or allgather, broadcast).

-----------------------------------------------------------------------------

### Pytorch集成

​       DistributedOptimizer(horovod/torch/optimizer.py): Allreduce operations are executed after each gradient is computed by ``loss.backward()`` in parallel with each other. The ``step()`` method ensures that all allreduce operations are finished before applying gradients to the model.   真正的实现是_DistributedOptimizer类。

​	\_DistributedOptimizer在初始化时通过 \_register_hooks方法为每个参数(pytorch的Tensor对象)注册一个hook。在反向传播过程中，当参数梯度计算出来后，hook被调用，触发AllReduce操作( \_allreduce_grad_async方法)。通过这种方式，horovod实现了WFBP。 在  \_allreduce_grad_async方法中，进行梯度压缩。\_allreduce_grad_async 方法调用 allreduce_async_ 方法(horovod/torch/mpi_ops.py)，通过allreduce完成梯度的同步。

​       在allreduce_async_ 方法中，调用C后端，进入horovod/torch/mpi_ops_v2.cc的DoAllreduce方法。在DoAllreduce中，调用EnqueueTensorAllreduce方法(operations.cc)，为传过来的梯度（一个Tensor对象）构造Request和相应的TensorTableEntry对象，并加入TensorQueue（分别加入TensorQueue的message_queue_ 和 tensor_table_）。

​	   Horovod初始化：mpi_ops.py导入了horovod/common/basics.py，完成初始化。通过basics.py，可以获取horovod环境的各种信息。

### Control Phase

​        不管用什么通信后端，都是通过MPI进行negotiation。rank 0被称为coordinator，它和其它rank进行coordination的过程由Request和Response(horovod/common/message.h中有详细注释)来表示。每个tensor一个Request，包含了对哪个tensor要进行什么操作的信息。一个Response里可能会有多个tensor，通过MPI_BCAST操作，发给所有的worker。每个Response对象对应一个collective operation操作。这些tensor放在fusion buffer里。

​      Horovod支持的后端：MPI_GPUAllreduce，NCCLHierarchicalAllreduce，DDLAllreduce，NCCLAllreduce，GlooAllreduce，CCLAllreduce，MPIAllreduce。

#### Controller

​     Controller(horovod/common/controller.cpp，controller.h有详细的注释)负责完成negotiation。每个通信后端，都实现了Controller的子类（如MPI有MPIController），但negotiation过程在父类Controller中完成。 每个Worker上都会运行一个Controller。
​	在coordinator node (rank zero)中，通过一个Vector（message_table_变量）保存着所有要进行allreduce的tensor，对某个tensor，当所有rank都同意对它进行allreduce时，就可以开始操作了（Controller::IncrementTensorCount）。
​	
 	Allreduce, Allgather and Broadcast三个操作的实现在horovod/common/operations.cc里，支持MPI, NCCL, CUDA/ROCm, Gloo, oneCCL, DDL等通信库。背景线程的实现也在operations.cc。背景线程的任务是完成系统的各种MPI操作（下文是horovod/common/global_state.h的注释）：MPI is a library that stores a lot of global per-program state and often requires running on a single thread. As a result, we have to have a single background thread responsible for all MPI operations, and communicate with that background thread through global state。

​	InitializeHorovodOnce：启动背景线程，无论该方法被调用多少次，只能执行一次。MPIController也是在这个方法里创建。

​    BackgroundThreadLoop：初始化controller(调用具体通信后端controller的DoInitialization方法)；设置参数的值，如fusion buffer大小与cycle周期、response cache大小（https://github.com/horovod/horovod/pull/902，https://github.com/horovod/horovod/issues/1305）等。

	horovod_init(horovod/common/operations.cc)->InitializeHorovodOnce(horovod/common/operations.cc)->BackgroundThreadLoop(horovod/common/operations.cc)->RunLoopOnce(horovod/common/operations.cc,后台线程的循环体)->ComputeResponseList(horovod/common/controller.cpp)->IncrementTensorCount(horovod/common/controller.cpp)

​	在背景线程每次循环中，worker把自身节点可以reduce的tensor通过一个MPI_GATHER操作发送给rank0（不是每个tensor发一次），rank0产生的response消息，也会经过合并，再通过MPI_BCAST操作，发送给worker。

#### operations

**BackgroundThreadLoop方法**：首先做了初始化，对各种参数和设置赋值。当每个节点上的rank数不同时(!is_homogeneous)，发出警告：Using different number of ranks per node might cause performance loss in hierarchical allgather and hierarchical allreduce. Consider assigning the same number of ranks to each node, or disabling hierarchical allgather and hierarchical allreduce。然后不停的调用RunLoopOnce。

**RunLoopOnce方法**：进入该方法之后，首先调用ComputeResponseList完成coordination，并获取本次循环要进行allreduce的tensor列表。然后进入AllReduece Phase，进行collective operation，真正的操作由PerformOperation方法完成。









   #### coordinator

判断某个Tensor是否可以进行allreduce：由controller的IncrementTensorCount方法完成。通过一个map(message_table_)保存coordinator收到的所有request消息。key是Tensor name, value是一个vector，长度是rank的个数，其中的元素是每个rank发来的request。当这个vector填满的时候，对应的tensor就可以进行allreduce了。 









​	Operation Manager

​		ExecuteOperation->ExecuteAllreduce。Horovod在编译时，会把集群支持的通信后端设置为enabled，ExecuteAllreduce方法遍历所有的horovod后端，找到第一个enabled后端并执行。	
​	

---------------------------------------------------------------------------------------
horovod/common/message.h中的Request，定义了RequestType，JOIN只适用于TF(因为operations.cc的EnqueueJoin方法会把一个请求的类型设置为JOIN，而这个方法只在tensorflow/mpi_ops.cc中被调用)?	



### AllReduce Phase

​       **PerformOperation方法**：完成具体的allreduce, broadcast等操作。首先初始化一个tensor fusion buffer。系统中有一个fusion buffer的Map，由deviceID和通信后端映射到buffer上。对于GPU上的数据，由于cuda 核函数通常是异步执行的，需要判断cuda event的状态，当状态为ready时，表明该tensor在GPU上的计算已经真正完成，可以进行allreduce。最后，对coordination阶段返回的ResponseList做循环，在协调阶段，已经对response进行了合并（参考Controller的FuseResponses方法），因此，对每个response进行一次allreduce操作（**为什么不同response中的tensor不能一起allreduce?**）。调用ExecuteOperation方法完成具体操作，根据response中的消息类型，调用相应的函数。

​       具体的通信操作定义在collective_operations.cc中，不同的通信后端分别实现，如mpi_operations.cc，gpu_operations.cc等。

​       **MPIAllreduce::Execute方法(mpi_operations.cc)**: 首先把response所对应的tensor数据从TensorTableEntry中拷贝到fusion buffer中（在PerformOperation中完成了fusion buffer的初始化），然后调用MPI_Allreduce，最后再把数据拷贝到TensorTableEntry中。



### Horovod已实现的优化

#### coordination阶段

1. response cache
2. response fuse

#### Allreduce阶段

1. tensor fusion



### TODO

1. Join操作，感觉是和多线程里的join类似，执行join的rank不参与当前allreduce操作，等待其它rank。什么时候会出现这种情况？
2. NCCL操作部分。在BackgroundThreadLoop方法里设置了GPU stream的个数，什么作用？